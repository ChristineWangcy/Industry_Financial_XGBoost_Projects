{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wafer test (XGBoost and Hyperparameters tuning).ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "5hFdspSDr9Or",
        "TDAJBsy7sI-v"
      ],
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "cXIojdRiDBd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats # check correlation\n",
        "from sklearn.neighbors import LocalOutlierFactor # find local outlier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from plotly import express as px\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "r9ZJe-DmIOfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data"
      ],
      "metadata": {
        "id": "_KHUkoitDFOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"25Wafer_10LayerVariation_SNR30db_train.csv\"\n",
        "test_file = \"1Wafer_10LayerVariation_SNR30db_validate.csv\"\n",
        "train_data = pd.read_csv(train_file, sep=',', skiprows=range(4), header=None)\n",
        "test_data = pd.read_csv(test_file, sep=',', skiprows=range(4),header=None)"
      ],
      "metadata": {
        "id": "eLkCSnu8I4Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Mining"
      ],
      "metadata": {
        "id": "VewKQ3eUDYYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### missing value"
      ],
      "metadata": {
        "id": "RADr591slN6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().sum().sum(), test_data.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "aHur9xn1MSCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dropna(inplace=True)\n",
        "train_data"
      ],
      "metadata": {
        "id": "psgx_Ta2__8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = train_data.drop(columns=[601])\n",
        "target = train_data[601]"
      ],
      "metadata": {
        "id": "iyS_yO-wHGpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "describes = train_data.describe()\n",
        "describes"
      ],
      "metadata": {
        "id": "FwyqaZXyDzmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_data_grouped = train_data.groupby([601]).mean()\n",
        "mean_data_grouped"
      ],
      "metadata": {
        "id": "3b31ulMbjDvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = [int(i) for i in mean_data_grouped.index.values]"
      ],
      "metadata": {
        "id": "HPm-WliPN6FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 131 classes from 200 to 1500"
      ],
      "metadata": {
        "id": "fUCeRKKP1IKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#describes.loc['mean'][:-1].plot.hist()\n",
        "mean_data = describes.loc['mean']\n",
        "mean_data[:-1].plot.box()"
      ],
      "metadata": {
        "id": "MokYN-66E8ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions: Is there a trend for every row data?"
      ],
      "metadata": {
        "id": "H2VuSO0nKFj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def row_data(row, df):\n",
        "  return pd.DataFrame(data=df.iloc[row,:], index = df.columns)\n",
        "\n",
        "def scatter(data, color):\n",
        "  return sns.scatterplot(data=data, palette=[color])"
      ],
      "metadata": {
        "id": "o2YO6p8RbNjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check mean, max, min and one sample trends"
      ],
      "metadata": {
        "id": "X7ALff7Qxj9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "describes_mean = pd.DataFrame(data=describes.loc['mean'][:-1], index = describes.columns[:-1])\n",
        "describes_max = pd.DataFrame(data=describes.loc['max'][:-1], index = describes.columns)\n",
        "describes_min = pd.DataFrame(data=describes.loc['min'][:-1], index = describes.columns)"
      ],
      "metadata": {
        "id": "stu2Jn4SL-AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scatter(describes_mean, 'red')\n",
        "scatter(describes_max, 'green')\n",
        "scatter(describes_min, 'blue')\n",
        "scatter(row_data(107, df_train), 'yellow')"
      ],
      "metadata": {
        "id": "gl39AQRJOf9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check the difference of trends with different depth"
      ],
      "metadata": {
        "id": "KhUYYLxDxc7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scatter(pd.DataFrame(mean_data_grouped.loc[200,:]), 'yellow')\n",
        "scatter(pd.DataFrame(mean_data_grouped.loc[600,:]), 'green')\n",
        "scatter(pd.DataFrame(mean_data_grouped.loc[700,:]), 'blue')\n",
        "scatter(pd.DataFrame(mean_data_grouped.loc[1500,:]), 'red')"
      ],
      "metadata": {
        "id": "TrfY1sXTK562"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion"
      ],
      "metadata": {
        "id": "P3wkrHO4xumW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is trend for row, and closer numbers of depth have similar graphs. \n",
        "\n",
        "Next step: find out outliers in each row not following the trend."
      ],
      "metadata": {
        "id": "D_cgSzqaqzac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other algorithms creating new features and reducing dementions tried"
      ],
      "metadata": {
        "id": "jjeDf4skQ4uR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Polynomial Regression(degree ranges from 2 to 60) for each row, use the coefficient data as training data. The prediction result is not better.\n",
        "\n",
        "2) Polynomial Regression(degree ranges from 2 to 60) for the mean value of grouped data by depth (mean_data_grouped), use the coefficient data as training data."
      ],
      "metadata": {
        "id": "bN1wH6wYRIk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers"
      ],
      "metadata": {
        "id": "BuSoVt647tXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ways tried filling outliers:"
      ],
      "metadata": {
        "id": "CXko6hbYSZ6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Find out outliers for each row using LocalOutlierFactor, replace outliers with the previous non-outlier value first and then the next non-outlier value in the row, because adjacent columns have similar data in each row shown in the graphs. "
      ],
      "metadata": {
        "id": "ORRDnuasxZtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier is data far away from local area, replace it with null\n",
        "\n",
        "lof = LocalOutlierFactor()\n",
        "def replace_outlier_None(df):\n",
        "  df_outlier_replaced = df.copy()\n",
        "  for i in range(len(df)):\n",
        "    group = target[i]\n",
        "    row_data = df.iloc[i,:].values.reshape(-1,1)\n",
        "    yhat = lof.fit_predict(row_data)\n",
        "    df_outlier_replaced.loc[i] = [df.loc[i,j] if yhat[j] == 1 else None \n",
        "                                  for j in range(len(yhat))]\n",
        "  return df_outlier_replaced\n",
        "\n",
        "# the following codes take long time to run, and the replace_outlier is not necessary, so they are commented\n",
        "'''\n",
        "replace_outlier_None = replace_outlier_None(df_train)\n",
        "replace_outlier = replace_outlier_None.fillna(method='ffill', axis=1)\n",
        "replace_outlier = replace_outlier.fillna(method='bfill', axis=1)\n",
        "'''"
      ],
      "metadata": {
        "id": "y-BnsdVCq5fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Set outlier = median of the column.\n",
        "\n",
        "3) Observe the graphs of samples with the greatest and the smallest depth, the main difference is from column 50 to 600, and most outliers lie before column 100 or after column 550, so remove the first 100 features or last 50 features. "
      ],
      "metadata": {
        "id": "kL2FAkJ8S8Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build models"
      ],
      "metadata": {
        "id": "1Z5ibbmCak0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qestions and Solutions"
      ],
      "metadata": {
        "id": "eAnpXNU91lTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: \n",
        "Classifier models or Regression Models?\n",
        "\n",
        "Thinking:\n",
        "1. The target is numeric numbers correlated with features.\n",
        "2. The number of samples 90,000+ is small for classification with 130+ categories.\n",
        "\n",
        "Answer:\n",
        "Regression model is better.\n",
        "\n",
        "\n",
        "Question: \n",
        "Predicted depth from regression model is continuous numbers, can I change it to integer depth such as 20, 500, 1500?\n",
        "\n",
        "Solution:\n",
        "Compare difference between predicted depth with each integer number of real depth, select the real depth with the smallest difference as predicted result."
      ],
      "metadata": {
        "id": "YWwlby2rzDH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "AWS_Nk3Nae1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mean absolute error"
      ],
      "metadata": {
        "id": "AkQKfHjnAR22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LinearRegression()\n",
        "X_train, X_val, y_train, y_val = train_test_split(df_train.values, target.values, test_size=0.2)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"train error: \", mean_absolute_error(y_train, lr.predict(X_train)))\n",
        "print(\"validation error: \", mean_absolute_error(y_val, lr.predict(X_val)))"
      ],
      "metadata": {
        "id": "rgvbBRqeaeOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare regression result using original dataset with result using outlier replaced datasets"
      ],
      "metadata": {
        "id": "VFA_Dayvd6e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this step needs replace_outlier file, the codes generating replace_outlier file are commented\n",
        "'''\n",
        "X_train, X_val, y_train, y_val = train_test_split(replace_outlier.values, target, test_size=0.2)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"train error: \", mean_absolute_error(y_train, lr.predict(X_train)))\n",
        "print(\"validation error: \", mean_absolute_error(y_val, lr.predict(X_val)))\n",
        "'''"
      ],
      "metadata": {
        "id": "nKF8AvSE3jtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using outlier replaced dataset does not improve model result, so we'll use original dataset"
      ],
      "metadata": {
        "id": "TsG-KYTdKbP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGB Cross Validation and Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "x8ziakKR1yKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0) n_estimator: [300, 350, 400, 450, 500, 550] - 300 and 400 are the best\n",
        "\n",
        "1) max_depth: range(4,12) ---> the best is 9\n",
        "\n",
        "2) min_child_weight: [3, 5, 7, 9] ---> the best is 7\n",
        "\n",
        "3) gamma: [0.07, 0.08, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6] --> the best is 0.07\n",
        "\n",
        "4) 'subsample': [0.6, 0.7, 0.8, 0.9], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9] --> the best is subsample=0.8, colsample_bytree=0.9\n",
        "\n",
        "5) 'reg_alpha': [0.02, 0.04, 0.05, 0.1, 1, 2, 3], 'reg_lambda': [0.01, 0.03, 0.04, 0.05, 0.06, 0.1, 1] --> the best is reg_alpha=0.04, reg_lambda=0.04\n",
        "\n",
        "6) 'learning_rate': [0.01, 0.05, 0.07, 0.09, 0.1, 0.15, 0.17, 0.2, 0.23] --> the best is 0.09"
      ],
      "metadata": {
        "id": "a28wYe4P95B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(df_train.values, target.values, test_size=0.2)\n",
        "params = {\n",
        "    'n_estimator': 400,\n",
        "    'max_depth': 9,\n",
        "    'min_child_weight': 7,\n",
        "    'gamma': 0.1, \n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'reg_alpha': 0.04,\n",
        "    'reg_lambda': 0.04,\n",
        "    'learning_rate': 0.09,\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'reg:gamma',\n",
        "    'lambda': 3,\n",
        "    'silent': 1,\n",
        "    'eta': 0.1,\n",
        "    'seed': 1000,\n",
        "    'nthread': 4\n",
        "}\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, y_train)\n",
        "gridsearch_params = [\n",
        "    (n_estimator, max_depth)\n",
        "    for (n_estimator in range(300,600,20),\n",
        "    max_depth in range(5,20,2))\n",
        "]\n",
        "min_mae = float(\"Inf\")\n",
        "best_params = None\n",
        "for n_estimator, max_depth in gridsearch_params:\n",
        "    print(\"CV with n_estimator={}, max_depth={}\".format(\n",
        "                             n_estimator, max_depth))\n",
        "    # Update our parameters\n",
        "    params['n_estimator'] = n_estimator\n",
        "    params['max_depth'] = max_depth\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=200,\n",
        "        seed=42,\n",
        "        nfold=3,\n",
        "        metrics={'mae'},\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "\n",
        "    # Update best MAE\n",
        "    mean_mae = cv_results['test-mae-mean'].min()\n",
        "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
        "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
        "    if mean_mae < min_mae:\n",
        "        min_mae = mean_mae\n",
        "        best_params = [n_estimator, max_depth]\n",
        "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))"
      ],
      "metadata": {
        "id": "KuQoA-JYjO65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##XGB Regression"
      ],
      "metadata": {
        "id": "azRivnoBer8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the best hyperparamers to build XGB model"
      ],
      "metadata": {
        "id": "c9eVm18r06zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(df_train.values, target.values, test_size=0.2)\n",
        "dtrain = xgb.DMatrix(X_train, y_train)\n",
        "\n",
        "params = {\n",
        "    'n_estimator': 400,\n",
        "    'max_depth': 9,\n",
        "    'min_child_weight': 7,\n",
        "    'gamma': 0.07, \n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'reg_alpha': 0.04,\n",
        "    'reg_lambda': 0.04,\n",
        "    'learning_rate': 0.09,\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'reg:gamma',\n",
        "    'lambda': 3,\n",
        "    'silent': 1,\n",
        "    'eta': 0.1,\n",
        "    'seed': 1000,\n",
        "    'nthread': 4\n",
        "}\n",
        "\n",
        "num_boost_rounds = 300 \n",
        "plst = params.items()\n",
        "model = xgb.train(plst, dtrain, num_boost_rounds)"
      ],
      "metadata": {
        "id": "io1TVdwaevSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mean absolute error"
      ],
      "metadata": {
        "id": "TSGXPcFpAZXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtrain = xgb.DMatrix(X_train)\n",
        "dval = xgb.DMatrix(X_val)\n",
        "\n",
        "print(\"train error: \", mean_absolute_error(y_train, model.predict(dtrain)))\n",
        "print(\"validation error: \", mean_absolute_error(y_val, model.predict(dval)))"
      ],
      "metadata": {
        "id": "TmjPwIXzjm_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions changing continuous numeric predicted depth into integer depth"
      ],
      "metadata": {
        "id": "_kRoIVNLx_ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_integer_depth(train_data, model):\n",
        "  '''use model to predict train_data, getting continous numeric data, \n",
        "  change continous numeric depth into integer depth in targets'''\n",
        "  data = [int(i) for i in mean_data_grouped.index.values]\n",
        "  indexes = [int(i) for i in mean_data_grouped.index.values]\n",
        "  groups = []\n",
        "  predict_test = model.predict(train_data)\n",
        "  for p in predict_test:\n",
        "    df_target = pd.DataFrame(data=data, index=indexes, columns=['test_target'])\n",
        "    df_target['test_target'] = abs(df_target['test_target'] - p)\n",
        "    groups.append(df_target.sort_values(by=['test_target']).index[0])\n",
        "  return groups"
      ],
      "metadata": {
        "id": "ltosJ08Amj5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_result(test_data, target, predict_groups):\n",
        "  '''get difference between prediction depth and real depth, \n",
        "  count number of samples for different difference'''\n",
        "  df = pd.DataFrame()\n",
        "  df['predict_depth'] = predict_groups\n",
        "  df['true_depth'] = target\n",
        "  df['difference'] = (df['predict_depth'] - df['true_depth'])\n",
        "  differences_counts = df['difference'].value_counts()\n",
        "  difference_result = pd.DataFrame({'predict_true_difference': differences_counts.index, 'number_of_samples': differences_counts.values})\n",
        "  abs_differences_counts = abs((df['predict_depth'] - df['true_depth'])).value_counts()\n",
        "  df_absolute_difference = pd.DataFrame({'absolute_difference': abs_differences_counts.index, 'number_of_samples': abs_differences_counts.values})\n",
        "  return df, difference_result, df_absolute_difference"
      ],
      "metadata": {
        "id": "4Sqj_7m6oehV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = xgb.DMatrix(test_data.drop(columns=[601]).values)\n",
        "predict_groups = get_integer_depth(train_data, model)"
      ],
      "metadata": {
        "id": "H-bWsdM7hh92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_df, difference_result, df_absolute_difference = prediction_result(test_data, test_data[601], predict_groups)\n",
        "difference_result['percentage'] = difference_result['number_of_samples'] * 100/len(test_data)\n",
        "difference_result['cum_percentage'] = difference_result['percentage'].cumsum(skipna=False)\n",
        "difference_result"
      ],
      "metadata": {
        "id": "3MXRt9SIo9hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(prediction_df['true_depth'])\n",
        "plt.hist(prediction_df['predict_depth'])"
      ],
      "metadata": {
        "id": "DjxRZW7eUqtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_df[['predict_depth', 'true_depth', 'difference']].to_csv('prediction.csv')"
      ],
      "metadata": {
        "id": "mNcS3-xKFzy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "difference_result.to_csv('prediction_performance.csv')"
      ],
      "metadata": {
        "id": "7V5GceDpSLEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "difference_result.to_csv('prediction.csv')"
      ],
      "metadata": {
        "id": "T8eDFMTXSDqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model, open('model', 'wb'))"
      ],
      "metadata": {
        "id": "cEZEX0D4FlN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "6sFdOiHV4hZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "34% of predicted depth is the same as the actual depth, 79% of predicted depth is 10 far from the actual depth, and 94% of predicted depth is 20 far from the actual depth."
      ],
      "metadata": {
        "id": "m4a-lnjx5Wr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations of predicted and true depth"
      ],
      "metadata": {
        "id": "6prPqxbMKFi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparation of predicted depth and true depth"
      ],
      "metadata": {
        "id": "5hFdspSDr9Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scatter plot\n",
        "prediction_df['absolute_difference<20'] = abs(prediction_df['difference']) < 20\n",
        "fig = px.scatter(prediction_df, x='true_depth', y='predict_depth', marginal_x='histogram', marginal_y='histogram', \n",
        "                 color='absolute_difference<20', trendline='ols')\n",
        "# hist grams\n",
        "fig.update_traces(histnorm='probability', selector={'type':'histogram'})\n",
        "# y = x line\n",
        "y = test_data.iloc[:, :-1].values\n",
        "fig.add_shape( type=\"line\", line=dict(dash='dash'), x0=y.min(), y0=y.min(), x1=y.max(), y1=y.max())"
      ],
      "metadata": {
        "id": "6y0d0eo4L6RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Percentage of samples with different distances from predicted depth to true depth"
      ],
      "metadata": {
        "id": "TDAJBsy7sI-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_absolute_difference = df_absolute_difference.sort_values(by=['absolute_difference'])\n",
        "df_absolute_difference['percentage'] = df_absolute_difference['number_of_samples'] * 100/len(test_data)\n",
        "df_absolute_difference['total_samples'] = df_absolute_difference['number_of_samples'].cumsum(skipna=False)\n",
        "df_absolute_difference['total_percentage'] = df_absolute_difference['percentage'].cumsum(skipna=False)"
      ],
      "metadata": {
        "id": "gcN7LdArmHzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(df_absolute_difference.iloc[:7,:], x='absolute_difference', y='total_samples',\n",
        "             hover_data=['total_percentage'], color='total_percentage',\n",
        "             labels={'total_samples':'total samples'}, height=400)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "b-RSDqUizVvH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}